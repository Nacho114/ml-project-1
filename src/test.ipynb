{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import loader\n",
    "import implementations as impl\n",
    "\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = loader.load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new_tX : column of ones followed by tX\n",
    "first_col = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((first_col, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_fun(n_iter, max_iters, loss, w):\n",
    "    print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                    bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import debugger \n",
    "\n",
    "\n",
    "debugger = debugger.Debugger(['loss', 'w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batch set to 1 as batch_size is None\n"
     ]
    }
   ],
   "source": [
    "from utils import debugger \n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 \n",
    "# 0.00000001 : after 50 iterations : 556513.5 loss\n",
    "# 0.0000001 : after 50 iterations : 97535 loss\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "debugger = debugger.Debugger(['loss', 'w'])\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = impl.least_squares_GD(y, new_tX, w_initial, max_iters, gamma, debugger=debugger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0,loss: 25250993.002723936\n",
      "iter 1,loss: 5078286.8206191715\n",
      "iter 2,loss: 1380135.2896036713\n",
      "iter 3,loss: 681465.0763505906\n",
      "iter 4,loss: 530507.864904485\n",
      "iter 5,loss: 481025.46312639304\n",
      "iter 6,loss: 451670.12028109556\n",
      "iter 7,loss: 427495.14221563423\n",
      "iter 8,loss: 405648.1429066689\n",
      "iter 9,loss: 385489.5057428916\n",
      "iter 10,loss: 366793.4166104151\n",
      "iter 11,loss: 349419.50308645045\n",
      "iter 12,loss: 333252.1289206743\n",
      "iter 13,loss: 318188.4765761683\n",
      "iter 14,loss: 304135.5895994201\n",
      "iter 15,loss: 291009.12068568764\n",
      "iter 16,loss: 278732.45840698923\n",
      "iter 17,loss: 267235.9846078639\n",
      "iter 18,loss: 256456.41141837416\n",
      "iter 19,loss: 246336.18323484124\n",
      "iter 20,loss: 236822.93615675488\n",
      "iter 21,loss: 227869.00913275647\n",
      "iter 22,loss: 219431.00181062333\n",
      "iter 23,loss: 211469.37460173457\n",
      "iter 24,loss: 203948.08690824397\n",
      "iter 25,loss: 196834.2698517054\n",
      "iter 26,loss: 190097.93019393494\n",
      "iter 27,loss: 183711.68245891674\n",
      "iter 28,loss: 177650.50655197003\n",
      "iter 29,loss: 171891.52843217284\n",
      "iter 30,loss: 166413.8216288278\n",
      "iter 31,loss: 161198.22760498998\n",
      "iter 32,loss: 156227.19316289094\n",
      "iter 33,loss: 151484.62325947033\n",
      "iter 34,loss: 146955.74775695207\n",
      "iter 35,loss: 142627.00077503466\n",
      "iter 36,loss: 138485.91143930785\n",
      "iter 37,loss: 134521.0049362274\n",
      "iter 38,loss: 130721.7128895852\n",
      "iter 39,loss: 127078.29216795508\n",
      "iter 40,loss: 123581.75131805822\n",
      "iter 41,loss: 120223.78389624073\n",
      "iter 42,loss: 116996.70804007363\n",
      "iter 43,loss: 113893.41168520632\n",
      "iter 44,loss: 110907.3028896486\n",
      "iter 45,loss: 108032.2647792253\n",
      "iter 46,loss: 105262.61467455998\n",
      "iter 47,loss: 102593.06700208144\n",
      "iter 48,loss: 100018.6996296271\n",
      "iter 49,loss: 97534.92330165528\n"
     ]
    }
   ],
   "source": [
    "debugger.print('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723936, w0=1.0006483246, w1=1.0278727712032\n",
      "Gradient Descent(1/49): loss=6230295.8271649005, w0=1.0004611384562112, w1=1.0091513490321606\n",
      "Gradient Descent(2/49): loss=5559897.929152627, w0=1.0002930279237274, w1=0.9878490547979485\n",
      "Gradient Descent(3/49): loss=5391747.897032267, w0=1.0007142520553163, w1=1.062161837316995\n",
      "Gradient Descent(4/49): loss=831528.9668088477, w0=1.0004884381006744, w1=1.010557251704568\n",
      "Gradient Descent(5/49): loss=760221.6709456951, w0=1.0006181605546516, w1=0.8809645201813028\n",
      "Gradient Descent(6/49): loss=1395123.2490015298, w0=1.0005460808504625, w1=0.8774494092471113\n",
      "Gradient Descent(7/49): loss=1422314.71079931, w0=1.0004695365338367, w1=0.8697541794637844\n",
      "Gradient Descent(8/49): loss=1490014.6981019764, w0=1.000349153881847, w1=0.8574312092928612\n",
      "Gradient Descent(9/49): loss=756158.6643079609, w0=1.0001711759939156, w1=0.8335177443146273\n",
      "Gradient Descent(10/49): loss=739984.9586463863, w0=1.0004018564735029, w1=0.6030679452069859\n",
      "Gradient Descent(11/49): loss=1050263.3234329778, w0=1.000346685019273, w1=0.5967726615935346\n",
      "Gradient Descent(12/49): loss=1076107.8200849055, w0=1.0001379205626109, w1=0.5763799231734068\n",
      "Gradient Descent(13/49): loss=330925.53722928243, w0=1.0000351211571445, w1=0.5664154740021473\n",
      "Gradient Descent(14/49): loss=316379.769676025, w0=1.0000629542341044, w1=0.5719337990056128\n",
      "Gradient Descent(15/49): loss=318509.10464210674, w0=0.999964576543586, w1=0.5612767405471237\n",
      "Gradient Descent(16/49): loss=365483.42138093873, w0=1.000056546095598, w1=0.569886378189173\n",
      "Gradient Descent(17/49): loss=283546.56329221104, w0=1.0000347636000018, w1=0.566754730579796\n",
      "Gradient Descent(18/49): loss=260523.36521388122, w0=0.9999456621207139, w1=0.5427446440575745\n",
      "Gradient Descent(19/49): loss=252024.2222250246, w0=0.9998867365464013, w1=0.5374326213844337\n",
      "Gradient Descent(20/49): loss=262658.1128193622, w0=0.9998738787944221, w1=0.5364517549169523\n",
      "Gradient Descent(21/49): loss=281257.932156826, w0=0.9998719550833183, w1=0.5363244398686773\n",
      "Gradient Descent(22/49): loss=284530.9336829076, w0=0.9998756575599852, w1=0.5367128333735108\n",
      "Gradient Descent(23/49): loss=278352.8266555665, w0=0.9998011317575902, w1=0.5309028763445991\n",
      "Gradient Descent(24/49): loss=255866.09139661535, w0=0.9997985579339672, w1=0.5306180261365918\n",
      "Gradient Descent(25/49): loss=259511.70314634303, w0=0.9998552560432276, w1=0.537310330765027\n",
      "Gradient Descent(26/49): loss=216312.25777322927, w0=0.9997467118265991, w1=0.5271388693132155\n",
      "Gradient Descent(27/49): loss=206191.16722829873, w0=0.9997293563427024, w1=0.5254294756377735\n",
      "Gradient Descent(28/49): loss=193442.98037677625, w0=0.9997764955697125, w1=0.47833738785469104\n",
      "Gradient Descent(29/49): loss=263426.3425236628, w0=0.999714540161574, w1=0.4705919085954432\n",
      "Gradient Descent(30/49): loss=261101.48074753, w0=0.9996604393155432, w1=0.4612803414808483\n",
      "Gradient Descent(31/49): loss=263224.1418467325, w0=0.9996756841773093, w1=0.4460507245763896\n",
      "Gradient Descent(32/49): loss=314947.7545619233, w0=0.9995891749687223, w1=0.4375473019183258\n",
      "Gradient Descent(33/49): loss=150984.37865346821, w0=0.9995545777389017, w1=0.4349651376706605\n",
      "Gradient Descent(34/49): loss=158647.08544649143, w0=0.9995545303796328, w1=0.4349612611724307\n",
      "Gradient Descent(35/49): loss=158686.30660834536, w0=0.9995418465541263, w1=0.4336585942414339\n",
      "Gradient Descent(36/49): loss=171821.3513591474, w0=0.9995491685086613, w1=0.434667017751728\n",
      "Gradient Descent(37/49): loss=163468.25238665653, w0=0.9995371749068866, w1=0.433023306622123\n",
      "Gradient Descent(38/49): loss=178128.9508150742, w0=0.9994833341462052, w1=0.4271223592514336\n",
      "Gradient Descent(39/49): loss=301011.6877383691, w0=0.9995509071279061, w1=0.43597935268191707\n",
      "Gradient Descent(40/49): loss=130074.54499681196, w0=0.9995567976120415, w1=0.43645598709621897\n",
      "Gradient Descent(41/49): loss=127076.59546775396, w0=0.9996086247289793, w1=0.38468069727544635\n",
      "Gradient Descent(42/49): loss=180315.95352166952, w0=0.9995578456745057, w1=0.37875051617780275\n",
      "Gradient Descent(43/49): loss=121955.4765704023, w0=0.9995384783934791, w1=0.37606200832753095\n",
      "Gradient Descent(44/49): loss=129418.00573371635, w0=0.999550520868745, w1=0.37692616431013853\n",
      "Gradient Descent(45/49): loss=123092.9883107764, w0=0.9995297090056057, w1=0.3747851438896804\n",
      "Gradient Descent(46/49): loss=136716.8970565316, w0=0.9995580011259445, w1=0.3772398814187623\n",
      "Gradient Descent(47/49): loss=117196.505238437, w0=0.9993845355151763, w1=0.35389141020935105\n",
      "Gradient Descent(48/49): loss=89639.54896056947, w0=0.9993511421218412, w1=0.3493535819890514\n",
      "Gradient Descent(49/49): loss=88957.7797317294, w0=0.9992966351918928, w1=0.33713459998174516\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 # loss = 90047\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = impl.least_squares_SGD(y, new_tX, w_initial, max_iters, gamma, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
