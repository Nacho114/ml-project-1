{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import loader\n",
    "import implementations as impl\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = loader.load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new_tX : column of ones followed by tX\n",
    "first_col = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((first_col, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batch set to 1 as batch_size is None\n",
      "Gradient Descent(0/49): loss=25250993.002723936, w0=1.0005252902751836, w1=0.917113831786522\n",
      "Gradient Descent(1/49): loss=5078286.8206191715, w0=1.0007221563962507, w1=0.870526726972049\n",
      "Gradient Descent(2/49): loss=1380135.2896036713, w0=1.0007795004675737, w1=0.8397516623891288\n",
      "Gradient Descent(3/49): loss=681465.0763505906, w0=1.0007778789428414, w1=0.8160379626385418\n",
      "Gradient Descent(4/49): loss=530507.864904485, w0=1.0007516355492305, w1=0.7956414700655866\n",
      "Gradient Descent(5/49): loss=481025.46312639304, w0=1.000715403244278, w1=0.7769539984607841\n",
      "Gradient Descent(6/49): loss=451670.12028109556, w0=1.0006754082495695, w1=0.7592789151494876\n",
      "Gradient Descent(7/49): loss=427495.14221563423, w0=1.0006342916098392, w1=0.7423088588458109\n",
      "Gradient Descent(8/49): loss=405648.1429066689, w0=1.0005931657384215, w1=0.7259029454630265\n",
      "Gradient Descent(9/49): loss=385489.5057428916, w0=1.000552491583502, w1=0.7099917176518077\n",
      "Gradient Descent(10/49): loss=366793.4166104151, w0=1.0005124527468108, w1=0.6945365827527599\n",
      "Gradient Descent(11/49): loss=349419.50308645045, w0=1.0004731150351436, w1=0.679512493015364\n",
      "Gradient Descent(12/49): loss=333252.1289206743, w0=1.0004344944922015, w1=0.6649005403999694\n",
      "Gradient Descent(13/49): loss=318188.4765761683, w0=1.000396586394995, w1=0.6506847811692634\n",
      "Gradient Descent(14/49): loss=304135.5895994201, w0=1.0003593776012327, w1=0.6368508654528372\n",
      "Gradient Descent(15/49): loss=291009.12068568764, w0=1.000322851796275, w1=0.6233854374713064\n",
      "Gradient Descent(16/49): loss=278732.45840698923, w0=1.0002869917124326, w1=0.6102758652092655\n",
      "Gradient Descent(17/49): loss=267235.9846078639, w0=1.000251780057752, w1=0.5975101113084861\n",
      "Gradient Descent(18/49): loss=256456.41141837416, w0=1.0002171998951976, w1=0.585076664860768\n",
      "Gradient Descent(19/49): loss=246336.18323484124, w0=1.0001832347882211, w1=0.5729644998082817\n",
      "Gradient Descent(20/49): loss=236822.93615675488, w0=1.0001498688474515, w1=0.5611630452936615\n",
      "Gradient Descent(21/49): loss=227869.00913275647, w0=1.0001170867359446, w1=0.5496621616784337\n",
      "Gradient Descent(22/49): loss=219431.00181062333, w0=1.0000848736574517, w1=0.5384521195226181\n",
      "Gradient Descent(23/49): loss=211469.37460173457, w0=1.000053215338108, w1=0.5275235803442636\n",
      "Gradient Descent(24/49): loss=203948.08690824397, w0=1.0000220980059453, w1=0.5168675786298479\n",
      "Gradient Descent(25/49): loss=196834.2698517054, w0=0.9999915083700736, w1=0.5064755048458793\n",
      "Gradient Descent(26/49): loss=190097.93019393494, w0=0.9999614336002955, w1=0.4963390893224032\n",
      "Gradient Descent(27/49): loss=183711.68245891674, w0=0.9999318613074453, w1=0.48645038693156734\n",
      "Gradient Descent(28/49): loss=177650.50655197003, w0=0.9999027795245538, w1=0.47680176250784656\n",
      "Gradient Descent(29/49): loss=171891.52843217284, w0=0.9998741766888569, w1=0.46738587696753503\n",
      "Gradient Descent(30/49): loss=166413.8216288278, w0=0.9998460416246319, w1=0.45819567409077494\n",
      "Gradient Descent(31/49): loss=161198.22760498998, w0=0.9998183635268295, w1=0.4492243679327215\n",
      "Gradient Descent(32/49): loss=156227.19316289094, w0=0.9997911319454709, w1=0.44046543083273315\n",
      "Gradient Descent(33/49): loss=151484.62325947033, w0=0.9997643367707709, w1=0.4319125819922815\n",
      "Gradient Descent(34/49): loss=146955.74775695207, w0=0.9997379682189569, w1=0.42355977659383354\n",
      "Gradient Descent(35/49): loss=142627.00077503466, w0=0.9997120168187458, w1=0.4154011954343689\n",
      "Gradient Descent(36/49): loss=138485.91143930785, w0=0.9996864733984511, w1=0.40743123504850914\n",
      "Gradient Descent(37/49): loss=134521.0049362274, w0=0.9996613290736872, w1=0.3996444982974751\n",
      "Gradient Descent(38/49): loss=130721.7128895852, w0=0.9996365752356436, w1=0.3920357854012556\n",
      "Gradient Descent(39/49): loss=127078.29216795508, w0=0.9996122035399001, w1=0.3846000853924834\n",
      "Gradient Descent(40/49): loss=123581.75131805822, w0=0.999588205895758, w1=0.3773325679715703\n",
      "Gradient Descent(41/49): loss=120223.78389624073, w0=0.999564574456062, w1=0.37022857574365353\n",
      "Gradient Descent(42/49): loss=116996.70804007363, w0=0.9995413016074889, w1=0.36328361681886134\n",
      "Gradient Descent(43/49): loss=113893.41168520632, w0=0.9995183799612811, w1=0.3564933577583106\n",
      "Gradient Descent(44/49): loss=110907.3028896486, w0=0.9994958023444033, w1=0.34985361684911087\n",
      "Gradient Descent(45/49): loss=108032.2647792253, w0=0.9994735617911011, w1=0.3433603576924703\n",
      "Gradient Descent(46/49): loss=105262.61467455998, w0=0.9994516515348445, w1=0.3370096830897751\n",
      "Gradient Descent(47/49): loss=102593.06700208144, w0=0.9994300650006349, w1=0.3307978292122568\n",
      "Gradient Descent(48/49): loss=100018.6996296271, w0=0.9994087957976607, w1=0.32472116004056606\n",
      "Gradient Descent(49/49): loss=97534.92330165528, w0=0.9993878377122832, w1=0.31877616206123893\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 \n",
    "# 0.00000001 : after 50 iterations : 556513.5 loss\n",
    "# 0.0000001 : after 50 iterations : 97535 loss\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = impl.least_squares_GD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723936, w0=1.0006095581, w1=1.0665826408211\n",
      "Gradient Descent(1/49): loss=7009918.521131915, w0=1.0007921531753066, w1=1.0955418545745628\n",
      "Gradient Descent(2/49): loss=4061843.624665742, w0=1.0011519974517538, w1=1.1540917569510194\n",
      "Gradient Descent(3/49): loss=1138446.8926269696, w0=1.0011644582611376, w1=1.1554059486734907\n",
      "Gradient Descent(4/49): loss=1165191.520466512, w0=1.0010593981068605, w1=1.1415251909700874\n",
      "Gradient Descent(5/49): loss=1153996.0177545005, w0=1.0008650235219143, w1=1.1216352284417108\n",
      "Gradient Descent(6/49): loss=1178555.6042593021, w0=1.0007422408141795, w1=1.1048374489792265\n",
      "Gradient Descent(7/49): loss=1146458.641695949, w0=1.00053177951724, w1=1.0850021031265582\n",
      "Gradient Descent(8/49): loss=2579322.8849956105, w0=1.0009544411783249, w1=0.6627631037026496\n",
      "Gradient Descent(9/49): loss=1237693.4395061845, w0=1.0006898452933495, w1=0.5850012836630837\n",
      "Gradient Descent(10/49): loss=421833.98051807465, w0=1.0007041707987328, w1=0.5706901037852532\n",
      "Gradient Descent(11/49): loss=400204.46501666046, w0=1.000760000300913, w1=0.5750062825988004\n",
      "Gradient Descent(12/49): loss=377348.67523054336, w0=1.0006814098674608, w1=0.5671589492281585\n",
      "Gradient Descent(13/49): loss=370193.41656518634, w0=1.0007311247940132, w1=0.5174937376023624\n",
      "Gradient Descent(14/49): loss=492934.82214811025, w0=1.000626030705216, w1=0.5057175245762707\n",
      "Gradient Descent(15/49): loss=500516.3932273623, w0=1.0005728289348017, w1=0.5011539831136781\n",
      "Gradient Descent(16/49): loss=350658.6782471545, w0=1.000467632499118, w1=0.4914168958303586\n",
      "Gradient Descent(17/49): loss=346328.7499031891, w0=1.0004337950331537, w1=0.5252205243287877\n",
      "Gradient Descent(18/49): loss=347622.15192875976, w0=1.000468115884184, w1=0.4909339941496076\n",
      "Gradient Descent(19/49): loss=441468.432020182, w0=1.0003607962777616, w1=0.4482860411142056\n",
      "Gradient Descent(20/49): loss=434000.7722465854, w0=1.0002985939199192, w1=0.4420251871902956\n",
      "Gradient Descent(21/49): loss=298574.4643261356, w0=1.0001942441829041, w1=0.42645599772817444\n",
      "Gradient Descent(22/49): loss=285698.2207778822, w0=1.000138434602944, w1=0.42045568654835336\n",
      "Gradient Descent(23/49): loss=291724.69690576853, w0=1.000119602519812, w1=0.41778126709445706\n",
      "Gradient Descent(24/49): loss=318261.54949085385, w0=1.0001826143847998, w1=0.42287168060749136\n",
      "Gradient Descent(25/49): loss=262304.3531680406, w0=1.0002331167476852, w1=0.3724198200850073\n",
      "Gradient Descent(26/49): loss=372085.16845397354, w0=1.0001421890239102, w1=0.3603397991983047\n",
      "Gradient Descent(27/49): loss=225212.72104683975, w0=1.0000900840801366, w1=0.3562670683731781\n",
      "Gradient Descent(28/49): loss=221558.85903503327, w0=1.0000873432113409, w1=0.3558693189752813\n",
      "Gradient Descent(29/49): loss=222263.49609448202, w0=1.0001064989251123, w1=0.357974570230199\n",
      "Gradient Descent(30/49): loss=225720.4135905431, w0=1.0001070140700699, w1=0.35802204547434163\n",
      "Gradient Descent(31/49): loss=226093.1888625396, w0=1.0000455873921936, w1=0.3533012209995188\n",
      "Gradient Descent(32/49): loss=222551.69277638127, w0=0.999967075042638, w1=0.3415162032818303\n",
      "Gradient Descent(33/49): loss=216332.05432823588, w0=0.999880751947822, w1=0.33372269531255405\n",
      "Gradient Descent(34/49): loss=209482.31962194617, w0=0.9998451953746359, w1=0.32898883538485657\n",
      "Gradient Descent(35/49): loss=194618.02785383177, w0=0.9998378580503123, w1=0.3278790283943019\n",
      "Gradient Descent(36/49): loss=197082.0255120872, w0=0.9998600191852693, w1=0.3302209063308887\n",
      "Gradient Descent(37/49): loss=192980.68724914305, w0=0.9998733355622217, w1=0.31691784575547327\n",
      "Gradient Descent(38/49): loss=202884.20474690534, w0=0.9998285243908772, w1=0.3015935454349661\n",
      "Gradient Descent(39/49): loss=204611.89336243726, w0=0.9998481348267706, w1=0.2820027199774498\n",
      "Gradient Descent(40/49): loss=193153.3031298972, w0=0.9997738128729038, w1=0.27230006322208966\n",
      "Gradient Descent(41/49): loss=182520.01393933687, w0=0.9996981446031248, w1=0.2623269852652159\n",
      "Gradient Descent(42/49): loss=174026.3823186799, w0=0.9996922576002867, w1=0.2616188612418272\n",
      "Gradient Descent(43/49): loss=175035.07827772485, w0=0.9995969784938018, w1=0.249343958674272\n",
      "Gradient Descent(44/49): loss=165218.0150695333, w0=0.9996096118711382, w1=0.2507522770462159\n",
      "Gradient Descent(45/49): loss=165810.90075966422, w0=0.9995092819938828, w1=0.23624267052742426\n",
      "Gradient Descent(46/49): loss=155980.52744104015, w0=0.9994886783771078, w1=0.23372035515820747\n",
      "Gradient Descent(47/49): loss=152628.83149470558, w0=0.9994948442881237, w1=0.23417575701402885\n",
      "Gradient Descent(48/49): loss=151006.2012979367, w0=0.9994151500375937, w1=0.22393042355438966\n",
      "Gradient Descent(49/49): loss=142272.40436745965, w0=0.9993789895052191, w1=0.21784514836371713\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 # loss = 90047\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = impl.least_squares_SGD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
