{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new_tX : column of ones followed by tX\n",
    "first_col = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((first_col, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    loss = 1/(2*N) * np.dot(e.T,e)\n",
    "    return loss\n",
    "    \n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N * np.dot(tx.T, e)\n",
    "\n",
    "# Linear regression using gradient descent\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        \n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=1.0005252902751836, w1=0.917113831786522\n",
      "Gradient Descent(1/49): loss=5078286.820619153, w0=1.0007221563962507, w1=0.870526726972049\n",
      "Gradient Descent(2/49): loss=1380135.289603642, w0=1.0007795004675737, w1=0.8397516623891288\n",
      "Gradient Descent(3/49): loss=681465.0763505857, w0=1.0007778789428414, w1=0.8160379626385416\n",
      "Gradient Descent(4/49): loss=530507.864904485, w0=1.0007516355492305, w1=0.7956414700655863\n",
      "Gradient Descent(5/49): loss=481025.46312639373, w0=1.000715403244278, w1=0.7769539984607841\n",
      "Gradient Descent(6/49): loss=451670.1202810963, w0=1.0006754082495695, w1=0.7592789151494876\n",
      "Gradient Descent(7/49): loss=427495.14221563534, w0=1.0006342916098392, w1=0.7423088588458108\n",
      "Gradient Descent(8/49): loss=405648.14290666924, w0=1.0005931657384215, w1=0.7259029454630265\n",
      "Gradient Descent(9/49): loss=385489.5057428921, w0=1.000552491583502, w1=0.7099917176518078\n",
      "Gradient Descent(10/49): loss=366793.4166104155, w0=1.0005124527468108, w1=0.69453658275276\n",
      "Gradient Descent(11/49): loss=349419.5030864508, w0=1.0004731150351436, w1=0.6795124930153641\n",
      "Gradient Descent(12/49): loss=333252.128920675, w0=1.0004344944922015, w1=0.6649005403999694\n",
      "Gradient Descent(13/49): loss=318188.4765761689, w0=1.000396586394995, w1=0.6506847811692635\n",
      "Gradient Descent(14/49): loss=304135.5895994208, w0=1.0003593776012327, w1=0.6368508654528372\n",
      "Gradient Descent(15/49): loss=291009.1206856883, w0=1.000322851796275, w1=0.6233854374713064\n",
      "Gradient Descent(16/49): loss=278732.4584069899, w0=1.0002869917124326, w1=0.6102758652092656\n",
      "Gradient Descent(17/49): loss=267235.98460786464, w0=1.000251780057752, w1=0.5975101113084862\n",
      "Gradient Descent(18/49): loss=256456.41141837498, w0=1.0002171998951976, w1=0.585076664860768\n",
      "Gradient Descent(19/49): loss=246336.18323484174, w0=1.0001832347882211, w1=0.5729644998082817\n",
      "Gradient Descent(20/49): loss=236822.93615675534, w0=1.0001498688474515, w1=0.5611630452936615\n",
      "Gradient Descent(21/49): loss=227869.00913275676, w0=1.0001170867359446, w1=0.5496621616784337\n",
      "Gradient Descent(22/49): loss=219431.0018106237, w0=1.0000848736574517, w1=0.5384521195226181\n",
      "Gradient Descent(23/49): loss=211469.37460173483, w0=1.000053215338108, w1=0.5275235803442634\n",
      "Gradient Descent(24/49): loss=203948.08690824418, w0=1.0000220980059453, w1=0.5168675786298476\n",
      "Gradient Descent(25/49): loss=196834.26985170585, w0=0.9999915083700736, w1=0.506475504845879\n",
      "Gradient Descent(26/49): loss=190097.9301939354, w0=0.9999614336002955, w1=0.4963390893224027\n",
      "Gradient Descent(27/49): loss=183711.6824589169, w0=0.9999318613074453, w1=0.4864503869315667\n",
      "Gradient Descent(28/49): loss=177650.50655197038, w0=0.9999027795245538, w1=0.47680176250784595\n",
      "Gradient Descent(29/49): loss=171891.52843217316, w0=0.9998741766888569, w1=0.4673858769675343\n",
      "Gradient Descent(30/49): loss=166413.82162882786, w0=0.9998460416246319, w1=0.4581956740907744\n",
      "Gradient Descent(31/49): loss=161198.22760499048, w0=0.9998183635268295, w1=0.4492243679327211\n",
      "Gradient Descent(32/49): loss=156227.19316289123, w0=0.9997911319454709, w1=0.44046543083273265\n",
      "Gradient Descent(33/49): loss=151484.6232594706, w0=0.9997643367707709, w1=0.4319125819922811\n",
      "Gradient Descent(34/49): loss=146955.7477569524, w0=0.9997379682189569, w1=0.42355977659383315\n",
      "Gradient Descent(35/49): loss=142627.00077503483, w0=0.9997120168187458, w1=0.41540119543436843\n",
      "Gradient Descent(36/49): loss=138485.911439308, w0=0.9996864733984511, w1=0.4074312350485086\n",
      "Gradient Descent(37/49): loss=134521.00493622757, w0=0.9996613290736872, w1=0.3996444982974746\n",
      "Gradient Descent(38/49): loss=130721.7128895855, w0=0.9996365752356436, w1=0.39203578540125505\n",
      "Gradient Descent(39/49): loss=127078.29216795533, w0=0.9996122035399001, w1=0.38460008539248275\n",
      "Gradient Descent(40/49): loss=123581.75131805858, w0=0.999588205895758, w1=0.3773325679715696\n",
      "Gradient Descent(41/49): loss=120223.78389624102, w0=0.999564574456062, w1=0.3702285757436529\n",
      "Gradient Descent(42/49): loss=116996.70804007375, w0=0.9995413016074889, w1=0.3632836168188608\n",
      "Gradient Descent(43/49): loss=113893.41168520653, w0=0.9995183799612811, w1=0.35649335775831\n",
      "Gradient Descent(44/49): loss=110907.30288964885, w0=0.9994958023444033, w1=0.34985361684911026\n",
      "Gradient Descent(45/49): loss=108032.26477922543, w0=0.9994735617911011, w1=0.3433603576924697\n",
      "Gradient Descent(46/49): loss=105262.61467456014, w0=0.9994516515348446, w1=0.33700968308977447\n",
      "Gradient Descent(47/49): loss=102593.06700208147, w0=0.999430065000635, w1=0.33079782921225626\n",
      "Gradient Descent(48/49): loss=100018.69962962723, w0=0.9994087957976608, w1=0.32472116004056545\n",
      "Gradient Descent(49/49): loss=97534.92330165538, w0=0.9993878377122833, w1=0.3187761620612383\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 \n",
    "# 0.00000001 : after 50 iterations : 556513.5 loss\n",
    "# 0.0000001 : after 50 iterations : 97535 loss\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    losses = []\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # Compute random (y,x) :\n",
    "        r = np.random.randint(0, high=len(y))\n",
    "        \n",
    "        minibatch_tx = np.array([tx[r,:]])\n",
    "        minibatch_y = np.array([y[r]])\n",
    "        \n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        w = w - gamma*gradient\n",
    "        ws.append(w)        \n",
    "\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=305.82470000000006, w1=27887.887679500007\n",
      "Gradient Descent(1/49): loss=1.618042084145149e+18, w0=-106185211.15219289, w1=-13386780237.389206\n",
      "Gradient Descent(2/49): loss=3.1106985360132167e+29, w0=53221611074555.86, w1=7580675132831938.0\n",
      "Gradient Descent(3/49): loss=7.780736298289699e+40, w0=8.435890564231789e+17, w1=1.044263510590188e+20\n",
      "Gradient Descent(4/49): loss=4.0160327490927934e+46, w0=1.0853529766662244e+22, w1=1.278409773152716e+24\n",
      "Gradient Descent(5/49): loss=3.2481830464429676e+57, w0=-5.442022532608851e+27, w1=-7.750095801769997e+29\n",
      "Gradient Descent(6/49): loss=8.154044680404138e+68, w0=2.7287466813280456e+33, w1=4.051288664879102e+35\n",
      "Gradient Descent(7/49): loss=2.047906428220624e+80, w0=-1.3687658803300232e+39, w1=-1.5382181115917353e+41\n",
      "Gradient Descent(8/49): loss=5.180093764704061e+91, w0=-8.061266334327467e+42, w1=-1.0638265638166964e+45\n",
      "Gradient Descent(9/49): loss=1.1812065961762282e+96, w0=-6.327047731448604e+46, w1=-5.3704613886506935e+48\n",
      "Gradient Descent(10/49): loss=1.101428280893784e+107, w0=2.1733332520402984e+52, w1=-2.1711667765550807e+55\n",
      "Gradient Descent(11/49): loss=8.532345333868669e+117, w0=-7.528905856453526e+57, w1=-2.4115820451343303e+59\n",
      "Gradient Descent(12/49): loss=1.5633068236731644e+129, w0=2.6304033103482387e+63, w1=3.4011451076818923e+65\n",
      "Gradient Descent(13/49): loss=1.2089763743773557e+140, w0=-9.205999892823117e+68, w1=-1.0128251889806201e+71\n",
      "Gradient Descent(14/49): loss=2.333044289675562e+151, w0=4.612640235530039e+74, w1=4.916428405497204e+76\n",
      "Gradient Descent(15/49): loss=5.867318305334583e+162, w0=-2.3106331766856784e+80, w1=-2.499111597372823e+82\n",
      "Gradient Descent(16/49): loss=1.4691633049869286e+174, w0=1.1440562787391013e+86, w1=-1.1429147806940653e+89\n",
      "Gradient Descent(17/49): loss=3.7228970961046627e+185, w0=-3.9393090338616136e+91, w1=-4.6338975723387475e+93\n",
      "Gradient Descent(18/49): loss=2.720903089740796e+196, w0=1.374773164847917e+97, w1=1.0946066442032333e+99\n",
      "Gradient Descent(19/49): loss=5.200745070009991e+207, w0=-4.781399880900768e+102, w1=-5.987131757298211e+104\n",
      "Gradient Descent(20/49): loss=3.977878014945048e+218, w0=1.6853728052236318e+108, w1=9.153225801634727e+109\n",
      "Gradient Descent(21/49): loss=4.977693714781038e+229, w0=-5.888199529500796e+113, w1=-6.160007298252891e+115\n",
      "Gradient Descent(22/49): loss=9.532721562757105e+240, w0=-9.393461144358789e+117, w1=-8.373046675812215e+119\n",
      "Gradient Descent(23/49): loss=4.470060867746304e+246, w0=-7.064315371063382e+120, w1=-8.055851920101343e+122\n",
      "Gradient Descent(24/49): loss=8.707479844182817e+254, w0=2.4519966308844963e+126, w1=-2.4495524970898597e+129\n",
      "Gradient Descent(25/49): loss=1.0845452866341213e+266, w0=-8.376536168552383e+131, w1=-9.175594471025543e+133\n",
      "Gradient Descent(26/49): loss=1.929197842121941e+277, w0=2.9095039289737263e+137, w1=5.564753155046809e+139\n",
      "Gradient Descent(27/49): loss=1.4750372110632785e+288, w0=9.006628518766786e+141, w1=1.1980156265136664e+144\n",
      "Gradient Descent(28/49): loss=2.2501476716812076e+295, w0=3.8041376428958393e+146, w1=3.7681430476228e+148\n",
      "Gradient Descent(29/49): loss=inf, w0=-1.323396958670264e+152, w1=-1.7353584181193325e+154\n",
      "Gradient Descent(30/49): loss=inf, w0=4.607994739223375e+157, w1=5.1582789317674846e+159\n",
      "Gradient Descent(31/49): loss=inf, w0=-2.3117933075825662e+163, w1=-3.1609392496690732e+165\n",
      "Gradient Descent(32/49): loss=inf, w0=8.08519436422471e+168, w1=6.652727891451393e+170\n",
      "Gradient Descent(33/49): loss=inf, w0=-2.8402664551999634e+174, w1=-3.23378792523807e+176\n",
      "Gradient Descent(34/49): loss=inf, w0=9.835579537233883e+179, w1=-9.825775565746461e+182\n",
      "Gradient Descent(35/49): loss=inf, w0=3.81424781230358e+184, w1=4.602810240932529e+186\n",
      "Gradient Descent(36/49): loss=inf, w0=-7.392485810529915e+188, w1=-5.917382083497773e+190\n",
      "Gradient Descent(37/49): loss=inf, w0=1.2847272390860575e+192, w1=4.110304380021316e+194\n",
      "Gradient Descent(38/49): loss=inf, w0=-4.378511034954169e+197, w1=4.3741494686487137e+200\n",
      "Gradient Descent(39/49): loss=inf, w0=-4.0810604849561405e+201, w1=-9.697142937610662e+203\n",
      "Gradient Descent(40/49): loss=inf, w0=-7.59526648957481e+205, w1=7.587166553713411e+208\n",
      "Gradient Descent(41/49): loss=inf, w0=-8.764720577824517e+209, w1=-9.413320405017964e+211\n",
      "Gradient Descent(42/49): loss=inf, w0=-3.814035428455431e+212, w1=-7.774677043142535e+214\n",
      "Gradient Descent(43/49): loss=inf, w0=1.3357203419033964e+218, w1=1.0675029707492657e+220\n",
      "Gradient Descent(44/49): loss=inf, w0=-6.68446858542184e+223, w1=-6.211276790877109e+225\n",
      "Gradient Descent(45/49): loss=inf, w0=2.3323496073284948e+229, w1=2.5692475090781185e+231\n",
      "Gradient Descent(46/49): loss=inf, w0=1.560217380363081e+233, w1=1.0322644361418313e+235\n",
      "Gradient Descent(47/49): loss=inf, w0=1.845116598709582e+237, w1=1.0142596059991032e+239\n",
      "Gradient Descent(48/49): loss=inf, w0=-9.240351845185564e+242, w1=-2.276401169727784e+245\n",
      "Gradient Descent(49/49): loss=inf, w0=-1.4050240457246994e+247, w1=-1.8380608692362317e+249\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 # loss = 90047\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
