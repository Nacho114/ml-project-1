{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new_tX : column of ones followed by tX\n",
    "first_col = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((first_col, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    loss = 1/(2*N) * np.dot(e.T,e)\n",
    "    return loss\n",
    "    \n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N * np.dot(tx.T, e)\n",
    "\n",
    "# Linear regression using gradient descent\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        \n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=1.0005252902751836, w1=0.917113831786522\n",
      "Gradient Descent(1/49): loss=5078286.820619153, w0=1.0007221563962507, w1=0.870526726972049\n",
      "Gradient Descent(2/49): loss=1380135.289603642, w0=1.0007795004675737, w1=0.8397516623891288\n",
      "Gradient Descent(3/49): loss=681465.0763505857, w0=1.0007778789428414, w1=0.8160379626385416\n",
      "Gradient Descent(4/49): loss=530507.864904485, w0=1.0007516355492305, w1=0.7956414700655863\n",
      "Gradient Descent(5/49): loss=481025.46312639373, w0=1.000715403244278, w1=0.7769539984607841\n",
      "Gradient Descent(6/49): loss=451670.1202810963, w0=1.0006754082495695, w1=0.7592789151494876\n",
      "Gradient Descent(7/49): loss=427495.14221563534, w0=1.0006342916098392, w1=0.7423088588458108\n",
      "Gradient Descent(8/49): loss=405648.14290666924, w0=1.0005931657384215, w1=0.7259029454630265\n",
      "Gradient Descent(9/49): loss=385489.5057428921, w0=1.000552491583502, w1=0.7099917176518078\n",
      "Gradient Descent(10/49): loss=366793.4166104155, w0=1.0005124527468108, w1=0.69453658275276\n",
      "Gradient Descent(11/49): loss=349419.5030864508, w0=1.0004731150351436, w1=0.6795124930153641\n",
      "Gradient Descent(12/49): loss=333252.128920675, w0=1.0004344944922015, w1=0.6649005403999694\n",
      "Gradient Descent(13/49): loss=318188.4765761689, w0=1.000396586394995, w1=0.6506847811692635\n",
      "Gradient Descent(14/49): loss=304135.5895994208, w0=1.0003593776012327, w1=0.6368508654528372\n",
      "Gradient Descent(15/49): loss=291009.1206856883, w0=1.000322851796275, w1=0.6233854374713064\n",
      "Gradient Descent(16/49): loss=278732.4584069899, w0=1.0002869917124326, w1=0.6102758652092656\n",
      "Gradient Descent(17/49): loss=267235.98460786464, w0=1.000251780057752, w1=0.5975101113084862\n",
      "Gradient Descent(18/49): loss=256456.41141837498, w0=1.0002171998951976, w1=0.585076664860768\n",
      "Gradient Descent(19/49): loss=246336.18323484174, w0=1.0001832347882211, w1=0.5729644998082817\n",
      "Gradient Descent(20/49): loss=236822.93615675534, w0=1.0001498688474515, w1=0.5611630452936615\n",
      "Gradient Descent(21/49): loss=227869.00913275676, w0=1.0001170867359446, w1=0.5496621616784337\n",
      "Gradient Descent(22/49): loss=219431.0018106237, w0=1.0000848736574517, w1=0.5384521195226181\n",
      "Gradient Descent(23/49): loss=211469.37460173483, w0=1.000053215338108, w1=0.5275235803442634\n",
      "Gradient Descent(24/49): loss=203948.08690824418, w0=1.0000220980059453, w1=0.5168675786298476\n",
      "Gradient Descent(25/49): loss=196834.26985170585, w0=0.9999915083700736, w1=0.506475504845879\n",
      "Gradient Descent(26/49): loss=190097.9301939354, w0=0.9999614336002955, w1=0.4963390893224027\n",
      "Gradient Descent(27/49): loss=183711.6824589169, w0=0.9999318613074453, w1=0.4864503869315667\n",
      "Gradient Descent(28/49): loss=177650.50655197038, w0=0.9999027795245538, w1=0.47680176250784595\n",
      "Gradient Descent(29/49): loss=171891.52843217316, w0=0.9998741766888569, w1=0.4673858769675343\n",
      "Gradient Descent(30/49): loss=166413.82162882786, w0=0.9998460416246319, w1=0.4581956740907744\n",
      "Gradient Descent(31/49): loss=161198.22760499048, w0=0.9998183635268295, w1=0.4492243679327211\n",
      "Gradient Descent(32/49): loss=156227.19316289123, w0=0.9997911319454709, w1=0.44046543083273265\n",
      "Gradient Descent(33/49): loss=151484.6232594706, w0=0.9997643367707709, w1=0.4319125819922811\n",
      "Gradient Descent(34/49): loss=146955.7477569524, w0=0.9997379682189569, w1=0.42355977659383315\n",
      "Gradient Descent(35/49): loss=142627.00077503483, w0=0.9997120168187458, w1=0.41540119543436843\n",
      "Gradient Descent(36/49): loss=138485.911439308, w0=0.9996864733984511, w1=0.4074312350485086\n",
      "Gradient Descent(37/49): loss=134521.00493622757, w0=0.9996613290736872, w1=0.3996444982974746\n",
      "Gradient Descent(38/49): loss=130721.7128895855, w0=0.9996365752356436, w1=0.39203578540125505\n",
      "Gradient Descent(39/49): loss=127078.29216795533, w0=0.9996122035399001, w1=0.38460008539248275\n",
      "Gradient Descent(40/49): loss=123581.75131805858, w0=0.999588205895758, w1=0.3773325679715696\n",
      "Gradient Descent(41/49): loss=120223.78389624102, w0=0.999564574456062, w1=0.3702285757436529\n",
      "Gradient Descent(42/49): loss=116996.70804007375, w0=0.9995413016074889, w1=0.3632836168188608\n",
      "Gradient Descent(43/49): loss=113893.41168520653, w0=0.9995183799612811, w1=0.35649335775831\n",
      "Gradient Descent(44/49): loss=110907.30288964885, w0=0.9994958023444033, w1=0.34985361684911026\n",
      "Gradient Descent(45/49): loss=108032.26477922543, w0=0.9994735617911011, w1=0.3433603576924697\n",
      "Gradient Descent(46/49): loss=105262.61467456014, w0=0.9994516515348446, w1=0.33700968308977447\n",
      "Gradient Descent(47/49): loss=102593.06700208147, w0=0.999430065000635, w1=0.33079782921225626\n",
      "Gradient Descent(48/49): loss=100018.69962962723, w0=0.9994087957976608, w1=0.32472116004056545\n",
      "Gradient Descent(49/49): loss=97534.92330165538, w0=0.9993878377122833, w1=0.3187761620612383\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 \n",
    "# 0.00000001 : after 50 iterations : 556513.5 loss\n",
    "# 0.0000001 : after 50 iterations : 97535 loss\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \n",
    "    batch_size = 1\n",
    "    \n",
    "    losses = []\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        \n",
    "        gradient = 0\n",
    "        for minibatch_y, minibatch_tx in helpers.batch_iter(y, tx, batch_size, 1, True):\n",
    "            gradient += compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            \n",
    "        gradient /= batch_size\n",
    "        \n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=0.9998871709, w1=0.983564185003\n",
      "Gradient Descent(1/49): loss=25154746.500849664, w0=0.9997714078103692, w1=0.9725471275259261\n",
      "Gradient Descent(2/49): loss=25023855.605147183, w0=1.0007214938102476, w1=1.083861103443661\n",
      "Gradient Descent(3/49): loss=430430.8032495237, w0=1.000653812418108, w1=1.0778286609622507\n",
      "Gradient Descent(4/49): loss=473993.31137771526, w0=1.0006824576329998, w1=1.0810616458506\n",
      "Gradient Descent(5/49): loss=419743.60435273283, w0=1.0006869958333802, w1=1.0815943897314508\n",
      "Gradient Descent(6/49): loss=415308.5935033907, w0=1.0006989659333587, w1=1.0696362598529439\n",
      "Gradient Descent(7/49): loss=414200.30678639415, w0=1.0008169272333112, w1=0.9517929212003889\n",
      "Gradient Descent(8/49): loss=679918.2095788867, w0=1.0007100133990223, w1=0.9419789796095206\n",
      "Gradient Descent(9/49): loss=382970.01449370873, w0=1.000686816234519, w1=0.939267741416717\n",
      "Gradient Descent(10/49): loss=401313.6223387828, w0=1.000571825594423, w1=0.9326989011012197\n",
      "Gradient Descent(11/49): loss=378501.00473863364, w0=1.0005802209146724, w1=0.933551479453828\n",
      "Gradient Descent(12/49): loss=369938.00840611185, w0=1.000314521691434, w1=0.9089660646283558\n",
      "Gradient Descent(13/49): loss=280134.31523089745, w0=1.0003545382927144, w1=0.868989479949279\n",
      "Gradient Descent(14/49): loss=361254.108302605, w0=1.000265796418435, w1=0.85643365838311\n",
      "Gradient Descent(15/49): loss=399425.93567217217, w0=1.0002696825403348, w1=0.8525514226052412\n",
      "Gradient Descent(16/49): loss=415623.924188849, w0=1.0001722050189572, w1=0.8379665434241681\n",
      "Gradient Descent(17/49): loss=217881.4961088818, w0=1.0002467841587233, w1=0.7634619827979421\n",
      "Gradient Descent(18/49): loss=375859.8729172548, w0=1.0002387659913372, w1=0.771472132016631\n",
      "Gradient Descent(19/49): loss=343580.6309566333, w0=1.0002419405972454, w1=0.7683007007143914\n",
      "Gradient Descent(20/49): loss=355875.6165452781, w0=1.0002544820021944, w1=0.7557718371702464\n",
      "Gradient Descent(21/49): loss=409853.40548646054, w0=1.0000643638343119, w1=0.7327331274680794\n",
      "Gradient Descent(22/49): loss=415590.24577626056, w0=1.000046532999106, w1=0.7505461318386909\n",
      "Gradient Descent(23/49): loss=331698.78952504415, w0=0.9999527364831584, w1=0.7398087756755933\n",
      "Gradient Descent(24/49): loss=170191.7563073239, w0=0.9999188492267096, w1=0.7736621448679173\n",
      "Gradient Descent(25/49): loss=170142.9974546713, w0=0.9998589377907484, w1=0.7665482010504461\n",
      "Gradient Descent(26/49): loss=163020.13603083492, w0=0.9998367849082116, w1=0.7649437785327197\n",
      "Gradient Descent(27/49): loss=180186.61242885867, w0=0.9998438646606365, w1=0.7659180374234164\n",
      "Gradient Descent(28/49): loss=169876.3821135202, w0=0.9998609148439478, w1=0.7673070988076114\n",
      "Gradient Descent(29/49): loss=159760.46899286646, w0=0.9997274774466853, w1=0.7551397431756267\n",
      "Gradient Descent(30/49): loss=136853.06765742862, w0=0.9996611023621113, w1=0.7467663935064328\n",
      "Gradient Descent(31/49): loss=132192.205400367, w0=0.9997332773147256, w1=0.6746636158446981\n",
      "Gradient Descent(32/49): loss=256246.9475714496, w0=0.9996485295808603, w1=0.6695568021497065\n",
      "Gradient Descent(33/49): loss=262939.92988802114, w0=0.9995511753895777, w1=0.6552759158304754\n",
      "Gradient Descent(34/49): loss=125605.88298569908, w0=0.9994905742185762, w1=0.652088233634626\n",
      "Gradient Descent(35/49): loss=121019.07383384384, w0=0.9994901117624917, w1=0.6520226679983283\n",
      "Gradient Descent(36/49): loss=121431.7522497648, w0=0.9994268507812234, w1=0.6445164362659432\n",
      "Gradient Descent(37/49): loss=112064.7461642742, w0=0.9994260116295005, w1=0.6443823398206225\n",
      "Gradient Descent(38/49): loss=112722.68250524136, w0=0.999384346912551, w1=0.6305852771263771\n",
      "Gradient Descent(39/49): loss=174454.05715163294, w0=0.9993905356010766, w1=0.6317573219028152\n",
      "Gradient Descent(40/49): loss=161220.76390629756, w0=0.9993476895042617, w1=0.6286724029321409\n",
      "Gradient Descent(41/49): loss=155996.73257758588, w0=0.9994441546731804, w1=0.5323036991823452\n",
      "Gradient Descent(42/49): loss=153849.00030269887, w0=0.9993901019386132, w1=0.5260447168358746\n",
      "Gradient Descent(43/49): loss=154774.61329482758, w0=0.9993747734286866, w1=0.5413578982524785\n",
      "Gradient Descent(44/49): loss=118431.81978566729, w0=0.9993339575105762, w1=0.5368818205928919\n",
      "Gradient Descent(45/49): loss=82623.11600274566, w0=0.9993201545961724, w1=0.5354338534631917\n",
      "Gradient Descent(46/49): loss=91196.05903360128, w0=0.9993104256959079, w1=0.5335974359781717\n",
      "Gradient Descent(47/49): loss=103468.88887425813, w0=0.999335481757215, w1=0.5363793854090398\n",
      "Gradient Descent(48/49): loss=85196.99175831447, w0=0.9993278218510117, w1=0.5354281169774663\n",
      "Gradient Descent(49/49): loss=83998.21755070779, w0=0.9993212048068965, w1=0.534581175032989\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 # loss = 90047\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
