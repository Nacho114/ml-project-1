{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((b, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    loss = 1/(2*N) * np.dot(np.transpose(e),e)\n",
    "    return loss\n",
    "    \n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N * np.dot(np.transpose(tx), e)\n",
    "\n",
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        \n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=1.0000525290275184, w1=0.9917113831786522\n",
      "Gradient Descent(1/49): loss=22505183.255464636, w0=1.0001017738134956, w1=0.9837857569912944\n",
      "Gradient Descent(2/49): loss=20064868.07483497, w0=1.000147923260036, w1=0.9762026344147593\n",
      "Gradient Descent(3/49): loss=17896017.367021717, w0=1.000191155434679, w1=0.968942702060656\n",
      "Gradient Descent(4/49): loss=15968392.097437894, w0=1.0002316381917278, w1=0.9619877528770707\n",
      "Gradient Descent(5/49): loss=14255121.951815097, w0=1.0002695297579487, w1=0.9553206227095495\n",
      "Gradient Descent(6/49): loss=12732330.047540246, w0=1.0003049792846803, w1=0.94892513050005\n",
      "Gradient Descent(7/49): loss=11378799.453650758, w0=1.000338127368284, w1=0.9427860219152376\n",
      "Gradient Descent(8/49): loss=10175676.86183618, w0=1.0003691065407463, w1=0.936888916207466\n",
      "Gradient Descent(9/49): loss=9106209.269675246, w0=1.0003980417321476, w1=0.931220256123058\n",
      "Gradient Descent(10/49): loss=8155509.9984128885, w0=1.0004250507066113, w1=0.9257672606831393\n",
      "Gradient Descent(11/49): loss=7310350.777292314, w0=1.0004502444732501, w1=0.9205178806722902\n",
      "Gradient Descent(12/49): loss=6558976.990522867, w0=1.0004737276735494, w1=0.9154607566797364\n",
      "Gradient Descent(13/49): loss=5890943.506473886, w0=1.0004955989465323, w1=0.9105851795466966\n",
      "Gradient Descent(14/49): loss=5296968.7961515505, w0=1.0005159512729862, w1=0.9058810530819059\n",
      "Gradient Descent(15/49): loss=4768805.30345974, w0=1.0005348722999483, w1=0.9013388589152418\n",
      "Gradient Descent(16/49): loss=4299124.256730729, w0=1.0005524446465826, w1=0.8969496233668406\n",
      "Gradient Descent(17/49): loss=3881413.3127106177, w0=1.0005687461925168, w1=0.892704886216125\n",
      "Gradient Descent(18/49): loss=3509885.6034125746, w0=1.0005838503496438, w1=0.888596671261788\n",
      "Gradient Descent(19/49): loss=3179398.9155126708, w0=1.0005978263183362, w1=0.8846174585700302\n",
      "Gradient Descent(20/49): loss=2885383.8734825603, w0=1.0006107393289674, w1=0.8807601583142344\n",
      "Gradient Descent(21/49): loss=2623780.123406617, w0=1.0006226508695837, w1=0.8770180861148155\n",
      "Gradient Descent(22/49): loss=2390979.6261752327, w0=1.000633618900519, w1=0.8733849397932151\n",
      "Gradient Descent(23/49): loss=2183776.268041326, w0=1.0006436980567022, w1=0.8698547774589465\n",
      "Gradient Descent(24/49): loss=1999321.084760478, w0=1.0006529398383635, w1=0.8664219968532433\n",
      "Gradient Descent(25/49): loss=1835082.473939015, w0=1.0006613927908043, w1=0.8630813158772508\n",
      "Gradient Descent(26/49): loss=1688810.8398837098, w0=1.0006691026738574, w1=0.8598277542368312\n",
      "Gradient Descent(27/49): loss=1558507.1771546518, w0=1.0006761126216297, w1=0.8566566161399487\n",
      "Gradient Descent(28/49): loss=1442395.1540340148, w0=1.0006824632930844, w1=0.8535634739862733\n",
      "Gradient Descent(29/49): loss=1338896.306006077, w0=1.0006881930139873, w1=0.8505441529921036\n",
      "Gradient Descent(30/49): loss=1246607.9927808002, w0=1.0006933379107132, w1=0.8475947166969707\n",
      "Gradient Descent(31/49): loss=1164283.810991137, w0=1.0006979320363796, w1=0.8447114533013631\n",
      "Gradient Descent(32/49): loss=1090816.1889922407, w0=1.0007020074897452, w1=0.8418908627879107\n",
      "Gradient Descent(33/49): loss=1025220.9206676413, w0=1.0007055945272927, w1=0.839129644781099\n",
      "Gradient Descent(34/49): loss=966623.4222293177, w0=1.000708721668882, w1=0.8364246871031629\n",
      "Gradient Descent(35/49): loss=914246.5200632126, w0=1.0007114157973447, w1=0.8337730549862351\n",
      "Gradient Descent(36/49): loss=867399.5990556494, w0=1.0007137022523684, w1=0.8311719809031175\n",
      "Gradient Descent(37/49): loss=825468.9598376225, w0=1.0007156049189962, w1=0.8286188549811975\n",
      "Gradient Descent(38/49): loss=787909.2502686926, w0=1.0007171463110507, w1=0.8261112159660707\n",
      "Gradient Descent(39/49): loss=754235.8514859141, w0=1.0007183476497745, w1=0.8236467427033448\n",
      "Gradient Descent(40/49): loss=724018.1121754313, w0=1.0007192289379614, w1=0.8212232461089088\n",
      "Gradient Descent(41/49): loss=696873.336571376, w0=1.000719809029835, w1=0.8188386615996579\n",
      "Gradient Descent(42/49): loss=672461.4422138216, w0=1.0007201056969215, w1=0.816491041958266\n",
      "Gradient Descent(43/49): loss=650480.2128519865, w0=1.0007201356901434, w1=0.8141785506071176\n",
      "Gradient Descent(44/49): loss=630661.0801911369, w0=1.0007199147983543, w1=0.8118994552679334\n",
      "Gradient Descent(45/49): loss=612765.3755678918, w0=1.000719457903515, w1=0.8096521219849724\n",
      "Gradient Descent(46/49): loss=596580.9992020493, w0=1.0007187790327063, w1=0.8074350094909609\n",
      "Gradient Descent(47/49): loss=581919.4605052527, w0=1.000717891407159, w1=0.8052466638960947\n",
      "Gradient Descent(48/49): loss=568613.2481092905, w0=1.0007168074884722, w1=0.8030857136815857\n",
      "Gradient Descent(49/49): loss=556513.49288197, w0=1.0007155390221814, w1=0.8009508649802918\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.00000001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = gradient_descent(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
