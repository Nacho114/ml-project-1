{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000,), (250000, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new_tX : column of ones followed by tX\n",
    "first_col = np.ones((tX.shape[0],1))\n",
    "new_tX = np.concatenate((first_col, tX), axis=1)\n",
    "new_tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    loss = 1/(2*N) * np.dot(e.T,e)\n",
    "    return loss\n",
    "    \n",
    "def compute_gradient(y, tx, w):\n",
    "    N = len(y)\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -1/N * np.dot(tx.T, e)\n",
    "\n",
    "# Linear regression using gradient descent\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        \n",
    "        w = w - gamma*gradient\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=1.0005252902751836, w1=0.917113831786522\n",
      "Gradient Descent(1/49): loss=5078286.820619153, w0=1.0007221563962507, w1=0.870526726972049\n",
      "Gradient Descent(2/49): loss=1380135.289603642, w0=1.0007795004675737, w1=0.8397516623891288\n",
      "Gradient Descent(3/49): loss=681465.0763505857, w0=1.0007778789428414, w1=0.8160379626385416\n",
      "Gradient Descent(4/49): loss=530507.864904485, w0=1.0007516355492305, w1=0.7956414700655863\n",
      "Gradient Descent(5/49): loss=481025.46312639373, w0=1.000715403244278, w1=0.7769539984607841\n",
      "Gradient Descent(6/49): loss=451670.1202810963, w0=1.0006754082495695, w1=0.7592789151494876\n",
      "Gradient Descent(7/49): loss=427495.14221563534, w0=1.0006342916098392, w1=0.7423088588458108\n",
      "Gradient Descent(8/49): loss=405648.14290666924, w0=1.0005931657384215, w1=0.7259029454630265\n",
      "Gradient Descent(9/49): loss=385489.5057428921, w0=1.000552491583502, w1=0.7099917176518078\n",
      "Gradient Descent(10/49): loss=366793.4166104155, w0=1.0005124527468108, w1=0.69453658275276\n",
      "Gradient Descent(11/49): loss=349419.5030864508, w0=1.0004731150351436, w1=0.6795124930153641\n",
      "Gradient Descent(12/49): loss=333252.128920675, w0=1.0004344944922015, w1=0.6649005403999694\n",
      "Gradient Descent(13/49): loss=318188.4765761689, w0=1.000396586394995, w1=0.6506847811692635\n",
      "Gradient Descent(14/49): loss=304135.5895994208, w0=1.0003593776012327, w1=0.6368508654528372\n",
      "Gradient Descent(15/49): loss=291009.1206856883, w0=1.000322851796275, w1=0.6233854374713064\n",
      "Gradient Descent(16/49): loss=278732.4584069899, w0=1.0002869917124326, w1=0.6102758652092656\n",
      "Gradient Descent(17/49): loss=267235.98460786464, w0=1.000251780057752, w1=0.5975101113084862\n",
      "Gradient Descent(18/49): loss=256456.41141837498, w0=1.0002171998951976, w1=0.585076664860768\n",
      "Gradient Descent(19/49): loss=246336.18323484174, w0=1.0001832347882211, w1=0.5729644998082817\n",
      "Gradient Descent(20/49): loss=236822.93615675534, w0=1.0001498688474515, w1=0.5611630452936615\n",
      "Gradient Descent(21/49): loss=227869.00913275676, w0=1.0001170867359446, w1=0.5496621616784337\n",
      "Gradient Descent(22/49): loss=219431.0018106237, w0=1.0000848736574517, w1=0.5384521195226181\n",
      "Gradient Descent(23/49): loss=211469.37460173483, w0=1.000053215338108, w1=0.5275235803442634\n",
      "Gradient Descent(24/49): loss=203948.08690824418, w0=1.0000220980059453, w1=0.5168675786298476\n",
      "Gradient Descent(25/49): loss=196834.26985170585, w0=0.9999915083700736, w1=0.506475504845879\n",
      "Gradient Descent(26/49): loss=190097.9301939354, w0=0.9999614336002955, w1=0.4963390893224027\n",
      "Gradient Descent(27/49): loss=183711.6824589169, w0=0.9999318613074453, w1=0.4864503869315667\n",
      "Gradient Descent(28/49): loss=177650.50655197038, w0=0.9999027795245538, w1=0.47680176250784595\n",
      "Gradient Descent(29/49): loss=171891.52843217316, w0=0.9998741766888569, w1=0.4673858769675343\n",
      "Gradient Descent(30/49): loss=166413.82162882786, w0=0.9998460416246319, w1=0.4581956740907744\n",
      "Gradient Descent(31/49): loss=161198.22760499048, w0=0.9998183635268295, w1=0.4492243679327211\n",
      "Gradient Descent(32/49): loss=156227.19316289123, w0=0.9997911319454709, w1=0.44046543083273265\n",
      "Gradient Descent(33/49): loss=151484.6232594706, w0=0.9997643367707709, w1=0.4319125819922811\n",
      "Gradient Descent(34/49): loss=146955.7477569524, w0=0.9997379682189569, w1=0.42355977659383315\n",
      "Gradient Descent(35/49): loss=142627.00077503483, w0=0.9997120168187458, w1=0.41540119543436843\n",
      "Gradient Descent(36/49): loss=138485.911439308, w0=0.9996864733984511, w1=0.4074312350485086\n",
      "Gradient Descent(37/49): loss=134521.00493622757, w0=0.9996613290736872, w1=0.3996444982974746\n",
      "Gradient Descent(38/49): loss=130721.7128895855, w0=0.9996365752356436, w1=0.39203578540125505\n",
      "Gradient Descent(39/49): loss=127078.29216795533, w0=0.9996122035399001, w1=0.38460008539248275\n",
      "Gradient Descent(40/49): loss=123581.75131805858, w0=0.999588205895758, w1=0.3773325679715696\n",
      "Gradient Descent(41/49): loss=120223.78389624102, w0=0.999564574456062, w1=0.3702285757436529\n",
      "Gradient Descent(42/49): loss=116996.70804007375, w0=0.9995413016074889, w1=0.3632836168188608\n",
      "Gradient Descent(43/49): loss=113893.41168520653, w0=0.9995183799612811, w1=0.35649335775831\n",
      "Gradient Descent(44/49): loss=110907.30288964885, w0=0.9994958023444033, w1=0.34985361684911026\n",
      "Gradient Descent(45/49): loss=108032.26477922543, w0=0.9994735617911011, w1=0.3433603576924697\n",
      "Gradient Descent(46/49): loss=105262.61467456014, w0=0.9994516515348446, w1=0.33700968308977447\n",
      "Gradient Descent(47/49): loss=102593.06700208147, w0=0.999430065000635, w1=0.33079782921225626\n",
      "Gradient Descent(48/49): loss=100018.69962962723, w0=0.9994087957976608, w1=0.32472116004056545\n",
      "Gradient Descent(49/49): loss=97534.92330165538, w0=0.9993878377122833, w1=0.3187761620612383\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 \n",
    "# 0.00000001 : after 50 iterations : 556513.5 loss\n",
    "# 0.0000001 : after 50 iterations : 97535 loss\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = least_squares_GD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    losses = []\n",
    "    ws = [initial_w]\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        \n",
    "        # Compute random (y,x) :\n",
    "        r = np.random.randint(0, high=len(y))\n",
    "        \n",
    "        minibatch_tx = np.array([tx[r,:]])\n",
    "        minibatch_y = np.array([y[r]])\n",
    "        \n",
    "        gradient = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        w = w - gamma*gradient\n",
    "        ws.append(w)        \n",
    "\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    \n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=25250993.002723932, w0=1.0006237109, w1=1.0508474074116\n",
      "Gradient Descent(1/49): loss=6712714.037657203, w0=1.000786203571106, w1=1.076137116771822\n",
      "Gradient Descent(2/49): loss=4113162.243207022, w0=1.0005993062414633, w1=1.0542185455406503\n",
      "Gradient Descent(3/49): loss=4027675.2467430485, w0=1.0005083236280032, w1=1.0460906137671935\n",
      "Gradient Descent(4/49): loss=3990369.4793077745, w0=1.0010024772581996, w1=0.5524311372010315\n",
      "Gradient Descent(5/49): loss=1715542.6490958217, w0=1.0007524479597554, w1=0.5457411032625584\n",
      "Gradient Descent(6/49): loss=669285.5305702217, w0=1.0006953268928507, w1=0.6028050491002466\n",
      "Gradient Descent(7/49): loss=747684.982660788, w0=1.000610099021007, w1=0.5940351863154003\n",
      "Gradient Descent(8/49): loss=1055221.0539398282, w0=1.0004411907544302, w1=0.5740359408279124\n",
      "Gradient Descent(9/49): loss=889442.2002954057, w0=1.000497700388365, w1=0.5175828165269648\n",
      "Gradient Descent(10/49): loss=682507.7796431184, w0=1.0006048881530611, w1=0.5338593857838203\n",
      "Gradient Descent(11/49): loss=564325.3313013618, w0=1.000483265471, w1=0.5185345629760814\n",
      "Gradient Descent(12/49): loss=552757.2678738467, w0=1.000447794594598, w1=0.5172285962487125\n",
      "Gradient Descent(13/49): loss=646220.6371454103, w0=1.0002116530404126, w1=0.4905807302835422\n",
      "Gradient Descent(14/49): loss=549190.3269192337, w0=1.0003405911636516, w1=0.4976295196047712\n",
      "Gradient Descent(15/49): loss=403796.2818480466, w0=1.0002099953776389, w1=0.48341142578577073\n",
      "Gradient Descent(16/49): loss=387968.97463702975, w0=1.0001500093385771, w1=0.47701379474775213\n",
      "Gradient Descent(17/49): loss=390698.891119363, w0=1.0001143635550362, w1=0.4718399162041527\n",
      "Gradient Descent(18/49): loss=324879.7514303653, w0=1.0000786711453884, w1=0.45590364791294263\n",
      "Gradient Descent(19/49): loss=327491.48238653684, w0=1.0001217812900547, w1=0.4572390277541245\n",
      "Gradient Descent(20/49): loss=331052.5728021168, w0=1.0000622970694122, w1=0.45282506064556466\n",
      "Gradient Descent(21/49): loss=303346.9691874364, w0=1.000088519149991, w1=0.45665080975786443\n",
      "Gradient Descent(22/49): loss=289071.0387079857, w0=0.9999917999520804, w1=0.44421320450254653\n",
      "Gradient Descent(23/49): loss=277364.11885377293, w0=0.9999027562584761, w1=0.4368841071256716\n",
      "Gradient Descent(24/49): loss=267759.88355753093, w0=0.9998487597942504, w1=0.43251789903191457\n",
      "Gradient Descent(25/49): loss=264578.3446218197, w0=0.9997677437464995, w1=0.42262316607195677\n",
      "Gradient Descent(26/49): loss=258176.5884035961, w0=0.9997755394915625, w1=0.42338986200741685\n",
      "Gradient Descent(27/49): loss=266797.612683301, w0=0.9998317706528526, w1=0.36721493187867493\n",
      "Gradient Descent(28/49): loss=423944.04712294554, w0=0.9996981008578462, w1=0.3509168411131342\n",
      "Gradient Descent(29/49): loss=416560.9413240675, w0=0.9995752540199355, w1=0.33617620333856124\n",
      "Gradient Descent(30/49): loss=177170.3941975194, w0=0.9995844734425476, w1=0.3374405549555923\n",
      "Gradient Descent(31/49): loss=173675.4875592817, w0=0.9996480609487381, w1=0.27391663627130114\n",
      "Gradient Descent(32/49): loss=273033.6120278689, w0=0.9995999536291008, w1=0.2684456313455394\n",
      "Gradient Descent(33/49): loss=172532.421074351, w0=0.9995556962043946, w1=0.2634353368093907\n",
      "Gradient Descent(34/49): loss=167027.78673776393, w0=0.9995396502934656, w1=0.26179932782289395\n",
      "Gradient Descent(35/49): loss=181950.4189043209, w0=0.999412265350122, w1=0.24681223446862297\n",
      "Gradient Descent(36/49): loss=159842.24441790546, w0=0.9993554923686802, w1=0.2383356309285002\n",
      "Gradient Descent(37/49): loss=140267.74583231017, w0=0.9993862686239906, w1=0.24194728604169263\n",
      "Gradient Descent(38/49): loss=126978.70034957396, w0=0.9993278727784731, w1=0.23481779565830616\n",
      "Gradient Descent(39/49): loss=123482.3116076111, w0=0.9992776080874778, w1=0.22833103675597008\n",
      "Gradient Descent(40/49): loss=134092.63554946787, w0=0.9992859707906357, w1=0.22903382996665989\n",
      "Gradient Descent(41/49): loss=125890.80108853351, w0=0.9992006566729633, w1=0.21735901953377534\n",
      "Gradient Descent(42/49): loss=114884.26700177432, w0=0.9991548164979844, w1=0.2131144485316008\n",
      "Gradient Descent(43/49): loss=111253.74587090661, w0=0.9990897164684835, w1=0.2068095757744668\n",
      "Gradient Descent(44/49): loss=105965.96844466546, w0=0.9990176193243187, w1=0.20101678653225374\n",
      "Gradient Descent(45/49): loss=96850.48558683471, w0=0.9990437957847055, w1=0.20357153053662527\n",
      "Gradient Descent(46/49): loss=90628.19568416888, w0=0.9990449285605438, w1=0.2037049828581419\n",
      "Gradient Descent(47/49): loss=91207.94634211555, w0=0.9990418691434406, w1=0.2067613405442489\n",
      "Gradient Descent(48/49): loss=89896.47002680693, w0=0.9990154224817198, w1=0.20524465094122393\n",
      "Gradient Descent(49/49): loss=90047.65044489024, w0=0.9990187552478973, w1=0.20567786388556633\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.0000001 # loss = 90047\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones((31,))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = least_squares_SGD(y, new_tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    # TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression using gradient descent or SGD\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
